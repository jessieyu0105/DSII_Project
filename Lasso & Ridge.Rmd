---
title: "Lasso & Ridge"
author: Shuwei Liu
date: 3/31/2019
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(plotmo)
library(caret)
```

```{r}
# import data
data = read_csv("./data/dataset.final.csv") %>% 
  select(-X1, -county_name, -state_abbr)

trRows <- createDataPartition(data$ale,
                              p = .75,
                              list = F)
#Train data
x1<-model.matrix(ale~., data)[trRows, -1]
y1<-data$ale[trRows]

#Test data
x2<-model.matrix(ale~., data)[-trRows, -1]
y2<-data$ale[-trRows]
```

## `glmnet()`

### Ridge

```{r}
# ridge
set.seed(3)
cv.ridge <- cv.glmnet(x1, y1, 
                      alpha = 0, 
                      lambda = exp(seq(-2, 2, length=200)), 
                      type.measure = "mse")
# cv.glmnet helps find the optimal lambda using cross-validation
plot(cv.ridge)
plot_glmnet(cv.ridge$glmnet.fit)
cv.ridge$lambda.min
```

```{r}
best.lambda <- cv.ridge$lambda.min 
coef_ridge = predict(cv.ridge, s = best.lambda, type="coefficients")
head(coef_ridge)
pred_rg = predict(cv.ridge, s = best.lambda, newx = x2, type = "response")
# test MSE
mean((pred_rg - y2)^2)
```

### Lasso

```{r}
set.seed(3)
cv.lasso <- cv.glmnet(x1, y1, 
                      alpha = 1, 
                      lambda = exp(seq(-6, -4, length=200)))
plot(cv.lasso) 
#trace plot
plot_glmnet(cv.lasso$glmnet.fit)
cv.lasso$lambda.min
```

```{r}
best.lambda = cv.lasso$lambda.min

coef_rg = 
  predict(cv.lasso, s=best.lambda, type="coefficients")

nrow(summary(coef_rg))
# There are 17 non-zero coefficient estimates

pred_rg = predict(cv.lasso, s=best.lambda, newx = x2, type = "response")
# test MSE
mean((pred_rg - y2)^2)
```


## `caret`

### Ridge

```{r}
set.seed(3)
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
ridge.fit <- train(x1, y1,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 0,  
                                            lambda = exp(seq(-2, 2, length=200))),
                     trControl = ctrl1)

plot(ridge.fit, xTrans = function(x) log(x)) 

ridge.fit$bestTune

coef_ridge = coef(ridge.fit$finalModel,ridge.fit$bestTune$lambda)
head(coef_ridge)

pred_rg = predict(ridge.fit$finalModel, s = ridge.fit$bestTune$lambda, newx = x2, type = "response")
# test MSE
mean((pred_rg - y2)^2)
```

### Lasso

```{r}
set.seed(3)
lasso.fit <- train(x1, y1,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-6, -4, length=200))),
                   trControl = ctrl1)
coef_rg = 
  predict(lasso.fit$finalModel, newx = x2, 
                        s = lasso.fit$bestTune$lambda, type="coefficients")

nrow(summary(coef_rg))
# There are 17 non-zero coefficient estimates

predy2.lasso <- predict(lasso.fit$finalModel, newx = x2, 
                        s = lasso.fit$bestTune$lambda, type = "response")
# test MSE
mean((predy2.lasso - y2)^2)
```

```{r}
set.seed(3)

lm.fit <- train(x1, y1,
                method = "lm",
                trControl = ctrl1)

resamp <- resamples(list(lasso = lasso.fit, 
                         ridge = ridge.fit, 
                         lm = lm.fit))
summary(resamp)
```

```{r}
bwplot(resamp, metric = "RMSE")
```

