---
title: "Machine Learning Models and Visualization of Black Box Models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(plotmo)
library(caret)
library(RANN)

library(splines)
library(mgcv)
library(readr)
library(pls)

library(pdp)
library(earth)
library(ranger)

theme_set(theme_bw())
options(scipen = 200) # do not use scientific notation
```

```{r}
# import data
data = read_csv("./data/dataset.final.csv") %>% 
  select(-X1, -county_name, -state_abbr)

set.seed(3)
trRows <- createDataPartition(data$ale,
                              p = .75,
                              list = F)


data[2,34] = NA
x <- model.matrix(ale~., data)[,-1]
x = x[match(rownames(data), rownames(x)),]

n = dim(data)[2]
for (i in 2 : n){
  x[,names(data)[i]] = data[[i]]
}
rownames(x) = rownames(data)

y <- data$ale


#Train data
x1<-as.matrix(x)[trRows,]
y1<-data$ale[trRows]

#Test data
x2<-as.matrix(x)[-trRows,]
y2<-data$ale[-trRows]
```


## `caret`

```{r}
ctrl <- trainControl(method = "cv")
```

### Ridge

```{r}
set.seed(3)

ridge.fit <- train(x1, y1,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(-5, 2, length=200))),
                   preProcess = c( "center", "scale", "knnImpute"),
                   trControl = ctrl)

plot(ridge.fit, xTrans = function(x1) log(x1)) 

ridge.fit$bestTune

coef_ridge = coef(ridge.fit$finalModel,ridge.fit$bestTune$lambda)
head(coef_ridge)

# test MSE
pred_rg = predict(ridge.fit, newdata = x2)
ridge_test = mean((pred_rg - y2)^2)
```

### Lasso

```{r}
set.seed(3)
lasso.fit <- train(x1, y1,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-4, -1, length=200))),
                   preProcess = c("center", "scale", "knnImpute"),
                   trControl = ctrl)

plot(lasso.fit, xTrans = function(x1) log(x1))

trans = preProcess(x1, method = c("center", "scale", "knnImpute"))

coef_lasso = 
  predict(lasso.fit$finalModel, newx =  predict(trans,x2), 
                        s = lasso.fit$bestTune$lambda, type="coefficients")

nrow(summary(coef_lasso))
# There are 15 non-zero coefficient estimates

# test MSE
pred_lasso <- predict(lasso.fit, newdata = x2)
lasso_test = mean((pred_lasso - y2)^2)
```

Obtain the number of non-zero coefficient estimates:

```{r}
# Re-fit the lasso model using the optimal lambda value
coef_lasso = coef_lasso %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  rename('coefficient' = '1') 

non_zero_coef = coef_lasso %>% 
  filter(coefficient != 0)

non_zero_coef %>% nrow() # 15 non-zero coefficient estimates

non_zero_coef %>% knitr::kable() 
```

### Least Square

```{r}
set.seed(3)

lm.fit <- train(x1, y1,
                method = "lm",
                preProcess = c("center", "scale", "knnImpute"),
                trControl = ctrl)

# test MSE
pred_lm <- predict(lm.fit, newdata = x2)
lm_test = mean((pred_lm - y2)^2)
```

### PCR

```{r PCR}
#PCR Model
set.seed(3)

pcr.fit <- train(x1, y1,
                 method = "pcr",
                 tuneLength = 33,
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)


ggplot(pcr.fit, highlight = TRUE)+ theme_bw()


# test MSE
pred_pcr = predict(pcr.fit, newdata = x2)
pcr_test = mean((pred_pcr - y2)^2)
```

### PLS

```{r pls}
set.seed(3)

pls.fit <- train(x1, y1,
                 method = "pls",
                 tuneLength = 33,
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)


trans = preProcess(x1, method = c("center", "scale", "knnImpute"))
summary(pls.fit)

ggplot(pls.fit, highlight = TRUE)+ theme_bw()

# test MSE
pred_pls = predict(pls.fit, newdata = x2)
pls_test = mean((pred_pls - y2)^2)
```

### GAM

```{r gam}
set.seed(3)

gam.fit <- train(x, y,
                 method = "gam",
                 tuneLength = data.frame(method = "GCV.Cp", select = c("TRUE", "FALSE")),
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)
summary(gam.fit)
gam.fit$bestTune
plot(gam.fit$finalModel, pages = 4)

# test MSE
pred_gam = predict(gam.fit, newdata = x2)
gam_test = mean((pred_gam - y2)^2)
```

### MARS1

```{r}
# MARS
mars_grid <- expand.grid(degree = 1:2, 
                         nprune = 3:15)

set.seed(3)

mars.fit <- train(x1, y1,
                 method = "earth",
                 tuneGrid = mars_grid,
                 preProcess = c("center", "scale", "knnImpute"),
                 trControl = ctrl)

ggplot(mars.fit)
summary(mars.fit)
mars.fit$bestTune

coef(mars.fit$finalModel)

# test MSE
pred_mars = predict(mars.fit, newdata = x2)
# test MSE
mars_test = mean((pred_mars - y2)^2)
```

### KNN

```{r}
set.seed(3)

# To find the optimal tuning parameter: 

# First I tried `tuneGrid = data.frame(k = seq(1,300,by = 5))`, the optimal k is 21. The cross-validation RMSE keep rising as k increases from 21 to 300 by adding increment number 5 for each time.

# Then I tried `tuneGrid = data.frame(k = seq(1,50,by = 1))`, generate the sequence from 1 to 40 by adding increment number 1 for each time, the optimal k is 8
knn.fit <- train(x1, y1,
                 method = "knn",
                 tuneGrid = data.frame(k = seq(1,40,by = 1)),
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)

knn.fit$bestTune

ggplot(knn.fit)

# test MSE
pred.knn = predict(knn.fit, newdata = x2)
knn_test = mean((pred.knn - y2)^2)
```

## Regression Tree

```{r}
set.seed(3)
# tune over cp, method = "rpart"
rpart.fit <- train(x1, y1, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-9,-5, length = 20))), # (-6,-4), (-9,-5)
                   preProcess = c( "center", "scale", "knnImpute"),
                   trControl = ctrl)
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
rpart.fit$bestTune

# test MSE
predy2.rt <- predict(rpart.fit, newdata =x2)
rt_test = mean((predy2.rt - y2)^2) # 0.764
```

## Bagging

```{r}
bag.grid <- expand.grid(mtry = 33,
                       splitrule = "variance",
                       min.node.size = 1:15)
set.seed(3)
bag.fit <- train(x1, y1, 
                method = "ranger",
                tuneGrid = bag.grid,
                preProcess = c( "center", "scale", "knnImpute"),
                trControl = ctrl)

ggplot(bag.fit, highlight = TRUE)

bag.fit$bestTune

# test MSE
predy2.bag <- predict(bag.fit, newdata =x2)
bagging_test = mean((predy2.bag - y2)^2) # 0.605
```

## Random Forest

```{r}
rf.grid <- expand.grid(mtry = 1:33,
                       splitrule = "variance",
                       min.node.size = 1:15)
set.seed(3)
rf.fit <- train(x1, y1, 
                method = "ranger",
                tuneGrid = rf.grid,
                preProcess = c( "center", "scale", "knnImpute"), 
                importance = "impurity",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)

rf.fit$bestTune

barplot(sort(ranger::importance(rf.fit$finalModel), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(33))


# test MSE
pred_rf <- predict(rf.fit, newdata = x2)
rf_test = mean((pred_rf - y2)^2) # 0.595
```

### Boosting

```{r}
### 1st attempt ###
# gbm.grid <- expand.grid(
#   n.trees = c(2000,2500,3000,3500,4000,4500,5000),
#   interaction.depth = 2:10,
#   shrinkage = c(0.001,0.003,0.005), 
#   n.minobsinnode = 1)

# gbm.fit$bestTune
#    n.trees interaction.depth shrinkage n.minobsinnode
#      4500                10     0.003              1

gbm.grid4 <- expand.grid(  
  n.trees = seq(4300, 5000, by = 100), # number of trees
  interaction.depth = 8:16, # number of splits d in each tree
  shrinkage = c(0.003, 0.004, 0.005), # learnig rate              
  n.minobsinnode = 1) # the minimum number of obs in your node

set.seed(3)

gbm.fit4 <- train(x1, y1,
                 method = "gbm",
                 tuneGrid = gbm.grid4, 
                 trControl = ctrl, 
                 preProcess = c("center", "scale", "knnImpute"),  
                 verbose = FALSE)

gbm.fit4$bestTune

ggplot(gbm.fit4, highlight = T)

summary(gbm.fit4$finalModel, las = 2, cBars = 33, cex.names = 0.6)

# test MSE
pred.gbm4 = predict(gbm.fit4, newdata = x2)

gbm_test4 = mean((pred.gbm4 - y2)^2) 

```

```{r}
# Partial Dependence Plot (PDP) for Random Forest

pdp_df <- data.frame(x1, ale = y1)

pdp_rf_1 <- rf.fit %>% 
  partial(pred.var = "all_death", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Death Rate",
       y = "Partial Dependence Function")

pdp_rf_2 <- rf.fit %>%          # selected
  partial(pred.var = "poverty", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Percent Poverty",
       y = "Partial Dependence Function") 



pdp_rf_3 <- rf.fit %>%     # selected
  partial(pred.var = "no_hs_diploma", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1, aes(x*10)) +
  labs(x = "Percent with No High School Diploma",
       y = "Partial Dependence Function",
       title = "Random Forest")

pdp_rf_5 <- rf.fit %>% 
  partial(pred.var = "black", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Percent Black",
       y = "Partial Dependence Function")

pdp_rf_6 <- rf.fit %>% 
  partial(pred.var = "health_status", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Health Status",
       y = "Partial Dependence Function")



grid.arrange(pdp_rf_1, pdp_rf_2, pdp_rf_3, pdp_rf_5, pdp_rf_6, nrow = 2)


```

```{r}
# Partial Dependence Plot (PDP) for Boosting

pdp_df <- data.frame(x1, ale = y1)

pdp_gbm_1 <- gbm.fit4 %>% 
  partial(pred.var = "all_death", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Death Rate",
       y = "Partial Dependence Function")

pdp_gbm_2 <- gbm.fit4 %>%          # selected
  partial(pred.var = "poverty", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Percent Poverty",
       y = "Partial Dependence Function") 

pdp_gbm_3 <- gbm.fit4 %>% 
  partial(pred.var = "black", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Percent Black",
       y = "Partial Dependence Function")

pdp_gbm_4 <- gbm.fit4 %>% 
  partial(pred.var = "population_size", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Population Size",
       y = "Partial Dependence Function")

pdp_gbm_5 <- gbm.fit4 %>%     # selected
  partial(pred.var = "no_hs_diploma", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Percent with No High School Diploma",
       y = "Partial Dependence Function",
       title = "Boosting")

pdp_gbm_6 <- gbm.fit4 %>%     # selected
  partial(pred.var = "population density", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Population Density",
       y = "Partial Dependence Function")


grid.arrange(pdp_gbm_1, pdp_gbm_2, pdp_gbm_3, pdp_gbm_4, pdp_gbm_5, nrow = 2)

grid.arrange(pdp_rf_3, pdp_gbm_5, nrow = 2) # no_hs_diploma
grid.arrange(pdp_rf_2, pdp_gbm_2, nrow = 2) # poverty
grid.arrange(pdp_rf_5, pdp_gbm_3, nrow = 2) # black
```

The influence of the difference features on the predicted average life expectancy is visualized in the above figures.

The PDP plots above displays the average change in predicted ALE as we vary each variable while holding all other variables constant. 

[PDP interpretation example](https://christophm.github.io/interpretable-ml-book/pdp.html)

[ICE interpretation example](https://christophm.github.io/interpretable-ml-book/ice.html#ice)

```{r}
# ICE curve for Random Forest
ice_rf_1 <- rf.fit %>% 
  partial(pred.var = "all_death", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Death Rate",
       y = "Predicted Average Life Expectancy",
       title = "rf, centered") 

ice_rf_2 <- rf.fit %>% 
  partial(pred.var = "poverty", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Percent Poverty",
       y = "Predicted Average Life Expectancy",
       title = "rf, centered") 

ice_rf_3 <- rf.fit %>% 
  partial(pred.var = "no_hs_diploma", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Percent with No High School Diploma",
       y = "Predicted Average Life Expectancy",
       title = "rf, centered") 

ice_rf_4 <- rf.fit %>% 
  partial(pred.var = "disabled_medicare", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Percent Medicare Based on Disability",
       y = "Predicted Average Life Expectancy",
       title = "rf, centered") 

ice_rf_5 <- rf.fit %>% 
  partial(pred.var = "black", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Percent Black",
       y = "Predicted Average Life Expectancy",
       title = "rf, centered") 

ice_rf_6 <- rf.fit %>% 
  partial(pred.var = "health_status", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Health Status",
       y = "Predicted Average Life Expectancy",
       title = "rf, centered") 

grid.arrange(ice_rf_1,ice_rf_2,ice_rf_3,ice_rf_5,ice_rf_6, nrow = 2)
```

```{r}
# ICE curve for Boosting
ice_gbm_1 <- gbm.fit4 %>% 
  partial(pred.var = "all_death", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Death Rate",
       y = "Predicted Average Life Expectancy") 

ice_gbm_2 <- gbm.fit4 %>% 
  partial(pred.var = "poverty", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Percent Poverty",
       y = "Predicted Average Life Expectancy") 

ice_gbm_3 <- gbm.fit4 %>% 
  partial(pred.var = "black", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Percent Black",
       y = "Predicted Average Life Expectancy")

ice_gbm_4 <- gbm.fit4 %>% 
  partial(pred.var = "population_size", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Population Size",
       y = "Predicted Average Life Expectancy")

ice_gbm_5 <- gbm.fit4 %>% 
  partial(pred.var = "no_hs_diploma", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = x1, alpha = .1,
           center = TRUE) +
  labs(x = "Percent with No High School Diploma",
       y = "Predicted Average Life Expectancy") 

grid.arrange(ice_gbm_1,ice_gbm_2,ice_gbm_3,ice_gbm_4,ice_gbm_5, nrow = 2)
```



```{r}
# Plot the features in an explanation: plot_features()

# remain the county_name for data
data_county = read_csv("./data/dataset.final.csv") %>% 
  select(-X1) 

data_county_test <- data_county[-trRows, ]

# Check: if there are duplicate county
data_county_test %>% 
  group_by(county_name) %>% 
  count() %>% 
  arrange(desc(n)) # some state have counties with the same name

data_county_test %>% 
  group_by(county_name, state_abbr) %>% 
  count() %>% 
  arrange(desc(n)) 

data_county_test$county <- paste(data_county_test$county_name, data_county_test$state_abbr)

data_county_test = data_county_test %>% 
  select(-county_name, -state_abbr, -ale)

# Plot
library(lime)
# select the 10th county: Lamar
new_obs10 <- data_county_test[10, ] %>% 
  column_to_rownames("county") # row.names(new_obs) <- new_obs$county
# random forest
explainer.rf10 <- lime(data.frame(x1), rf.fit) # lime(predictors, model.fit)
explanation.rf10 <- explain(new_obs10, explainer.rf10, n_features = 8) # n_features = 8 or 5
rf_10 = plot_features(explanation.rf10) +
  ggtitle("Random Forest")
# boosting
explainer.gbm10 <- lime(data.frame(x1), gbm.fit4) # lime(predictors, model.fit)
explanation.gbm10 <- explain(new_obs10, explainer.gbm, n_features = 8) # n_features = 8 or 5
gbm_10 = plot_features(explanation.gbm10) +
  ggtitle("Boosting")
# grid.arrange
grid.arrange(rf_10, gbm_10, nrow = 1)

# select the 10th county: Lamar
# 10: Lamar in AL
# 15: Montgomery in AL
# 200: Guthrie in IA
# 300: Clare in MI
# 700: Fairfax in VA
new_obs3 <- data_county_test[700, ] %>% 
  column_to_rownames("county") # row.names(new_obs) <- new_obs$county
# random forest
explainer.rf3 <- lime(data.frame(x1), rf.fit) # lime(predictors, model.fit)
explanation.rf3 <- explain(new_obs3, explainer.rf3, n_features = 8) # n_features = 8 or 5
rf_3 = plot_features(explanation.rf3) +
  ggtitle("Random Forest")
# boosting
explainer.gbm3 <- lime(data.frame(x1), gbm.fit4) # lime(predictors, model.fit)
explanation.gbm3 <- explain(new_obs3, explainer.gbm, n_features = 8) # n_features = 8 or 5
gbm_3 = plot_features(explanation.gbm3) +
  ggtitle("Boosting")
# grid.arrange
grid.arrange(rf_3, gbm_3, nrow = 1)

```


```{r}
set.seed(3)


resamp <- resamples(list(ls = lm.fit,
                         ridge = ridge.fit, 
                         lasso = lasso.fit, 
                         pcr = pcr.fit,
                         pls = pls.fit,   
                         gam = gam.fit,
                         mars = mars.fit,
                         knn = knn.fit,
                         regression_tree = rpart.fit,
                         bagging = bag.fit,
                         random_forest = rf.fit,
                         boosting = gbm.fit4))

summary(resamp)
```

```{r}
# Compare test MSE
tibble(ls = lm_test,
       ridge = ridge_test,
       lasso = lasso_test,
       pcr = pcr_test,
       pls = pls_test,
       gam = gam_test,
       mars = mars_test, 
       knn = knn_test,
       regression_test = rt_test,
       bagging = bagging_test,
       random_forest = rf_test,
       boosting = gbm_test4
) %>% 
  gather(key = "model", value = "test_MSE", ls:boosting)
```




