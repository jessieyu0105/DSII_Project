---
title: "boosting and knn"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(plotmo)
library(caret)
library(RANN)

library(splines)
library(mgcv)
library(readr)
library(pls)

library(pdp)
library(earth)
```

```{r}
# import data
data = read_csv("./data/dataset.final.csv") %>% 
  select(-X1, -county_name, -state_abbr)

set.seed(3)
trRows <- createDataPartition(data$ale,
                              p = .75,
                              list = F)


data[2,34] = NA
x <- model.matrix(ale~., data)[,-1]
x = x[match(rownames(data), rownames(x)),]

n = dim(data)[2]
for (i in 2 : n){
  x[,names(data)[i]] = data[[i]]
}
rownames(x) = rownames(data)

y <- data$ale


#Train data
x1<-as.matrix(x)[trRows,]
y1<-data$ale[trRows]

#Test data
x2<-as.matrix(x)[-trRows,]
y2<-data$ale[-trRows]
```


## `caret`

```{r}
ctrl <- trainControl(method = "cv")
```

### Ridge

```{r}
set.seed(3)

ridge.fit <- train(x1, y1,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(-5, 2, length=200))),
                   preProcess = c( "center", "scale", "knnImpute"),
                   trControl = ctrl)

plot(ridge.fit, xTrans = function(x1) log(x1)) 

trans = preProcess(x1, method = c("center", "scale", "knnImpute"))

ridge.fit$bestTune

coef_ridge = coef(ridge.fit$finalModel,ridge.fit$bestTune$lambda)
head(coef_ridge)

pred_rg = predict(ridge.fit$finalModel, s = ridge.fit$bestTune$lambda, newx = predict(trans, x2),
                  type = "response")
# test MSE
mean((pred_rg - y2)^2)
```

### Lasso

```{r}
set.seed(3)
lasso.fit <- train(x1, y1,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-4, -1, length=200))),
                   preProcess = c("center", "scale", "knnImpute"),
                   trControl = ctrl)

plot(lasso.fit, xTrans = function(x1) log(x1))

trans = preProcess(x1, method = c("center", "scale", "knnImpute"))

coef_lasso = 
  predict(lasso.fit$finalModel, newx =  predict(trans,x2), 
                        s = lasso.fit$bestTune$lambda, type="coefficients")

nrow(summary(coef_lasso))
# There are 15 non-zero coefficient estimates

predy2.lasso <- predict(lasso.fit$finalModel, newx = predict(trans,x2), 
                        s = lasso.fit$bestTune$lambda, type = "response")
# test MSE
mean((predy2.lasso - y2)^2)
```

Obtain the number of non-zero coefficient estimates:

```{r}
# Re-fit the lasso model using the optimal lambda value
coef_lasso = coef_lasso %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  rename('coefficient' = '1') 

non_zero_coef = coef_lasso %>% 
  filter(coefficient != 0)

non_zero_coef %>% nrow() # 15 non-zero coefficient estimates

non_zero_coef %>% knitr::kable() 
```

### Least Square

```{r}
set.seed(3)

lm.fit <- train(x1, y1,
                method = "lm",
                preProcess = c("center", "scale", "knnImpute"),
                trControl = ctrl)
```

### PCR

```{r PCR}
#PCR Model
set.seed(3)

pcr.fit <- train(x1, y1,
                 method = "pcr",
                 tuneLength = 33,
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)


trans = preProcess(x1, method = c("center", "scale", "knnImpute"))



predy2.pcr = predict(pcr.fit$finalModel, newdata = predict(trans, x2), ncomp = pcr.fit$bestTune$ncomp)
# test MSE
mean((predy2.pcr - y2)^2)
ggplot(pcr.fit, highlight = TRUE)+ theme_bw()
```

### PLS

```{r pls}
set.seed(3)

pls.fit <- train(x1, y1,
                 method = "pls",
                 tuneLength = 33,
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)


trans = preProcess(x1, method = c("center", "scale", "knnImpute"))
summary(pls.fit)

predy2.pls = predict(pls.fit$finalModel, newdata = predict(trans, x2), ncomp = pls.fit$bestTune$ncomp)
# test MSE
mean((predy2.pls - y2)^2)
ggplot(pls.fit, highlight = TRUE)+ theme_bw()
```

### GAM

```{r gam}
set.seed(3)

gam.fit <- train(x, y,
                 method = "gam",
                 tuneLength = data.frame(method = "GCV.Cp", select = c("TRUE", "FALSE")),
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)
summary(gam.fit)
gam.fit$bestTune
plot(gam.fit$finalModel, pages = 4)
```

### MARS1

```{r}
# MARS
mars_grid <- expand.grid(degree = 1:2, 
                         nprune = 3:15)

set.seed(3)

mars.fit <- train(x1, y1,
                 method = "earth",
                 tuneGrid = mars_grid,
                 preProcess = c("center", "scale", "knnImpute"),
                 trControl = ctrl)

ggplot(mars.fit)
summary(mars.fit)
mars.fit$bestTune

coef(mars.fit$finalModel)
```

### KNN

```{r}
set.seed(3)

# To find the optimal tuning parameter: 

# First I tried `tuneGrid = data.frame(k = seq(1,300,by = 5))`, the optimal k is 21. The cross-validation RMSE keep rising as k increases from 21 to 300 by adding increment number 5 for each time.

# Then I tried `tuneGrid = data.frame(k = seq(1,50,by = 1))`, generate the sequence from 1 to 40 by adding increment number 1 for each time, the optimal k is 8
knn.fit <- train(x1, y1,
                 method = "knn",
                 tuneGrid = data.frame(k = seq(1,40,by = 1)),
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)

knn.fit$bestTune

ggplot(knn.fit)

# test MSE
trans = preProcess(x1, method = c("center", "scale", "knnImpute"))
pred.knn = predict(knn.fit$finalModel, 
                   k = knn.fit$bestTune$k, 
                   newdata = predict(trans, x2),
                   type = "response")
mean((pred.knn - y2)^2)
```


### Boosting

```{r}
### 1st attempt ###
# gbm.grid <- expand.grid(
#   n.trees = c(2000,2500,3000,3500,4000,4500,5000),
#   interaction.depth = 2:10,
#   shrinkage = c(0.001,0.003,0.005), 
#   n.minobsinnode = 1)

# gbm.fit$bestTune
#    n.trees interaction.depth shrinkage n.minobsinnode
#      4500                10     0.003              1


### 2nd attempt ###
# gbm.grid <- expand.grid(  
#   n.trees = c(4300,4500,4700,4900,5100,5300,5500), # number of trees
#   interaction.depth = 9:15, # number of splits d in each tree
#   shrinkage = c(0.003,0.004,0.005), # learnig rate 
#   n.minobsinnode = 1) # the minimum number of obs in your node

# gbm.fit$bestTune
#   n.trees interaction.depth shrinkage n.minobsinnode
#      4700                14     0.004             1

### 3rd attempt ###
gbm.grid <- expand.grid(  
  n.trees = seq(4300, 5000, by = 100), # number of trees
  interaction.depth = 8:16, # number of splits d in each tree
  shrinkage = c(0.003, 0.004, 0.005), # learnig rate 
  n.minobsinnode = 1) # the minimum number of obs in your node

set.seed(3)

gbm.fit <- train(x1, y1,
                 method = "gbm",
                 tuneGrid = gbm.grid, 
                 trControl = ctrl,
                 verbose = FALSE)

gbm.fit$bestTune

ggplot(gbm.fit, highlight = T)

summary(gbm.fit$finalModel, las = 2, cBars = 33, cex.names = 0.6)

# test MSE
trans = preProcess(x1, method = c("center", "scale", "knnImpute"))
pred.gbm = predict(gbm.fit$finalModel, 
                   n.trees = gbm.fit$bestTune$n.trees, 
                   interaction.depth = gbm.fit$bestTune$interaction.depth,
                   shrinkage = gbm.fit$bestTune$shrinkage,
                   newdata = predict(trans, x2),
                   type = "response")
mean((pred.gbm - y2)^2) # 6.551068
```

```{r}
set.seed(3)

resamp <- resamples(list(ls = lm.fit,
                         ridge = ridge.fit, 
                         lasso = lasso.fit, 
                         pcr = pcr.fit,
                         pls = pls.fit,   # ç¼ºgam
                         mars = mars.fit,
                         knn = knn.fit,
                         boosting = gbm.fit))


summary(resamp)
```


