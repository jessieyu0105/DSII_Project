---
title: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  echo = FALSE
)

library(tidyverse)
library(glmnet)
library(plotmo)
library(caret)
library(RANN)

library(splines)
library(mgcv)
library(readr)
library(pls)

library(pdp)
library(earth)

library(corrplot)

library(ranger)

theme_set(theme_bw() + theme(legend.position = "bottom"))

options(scipen = 200) # do not use scientific notation

```

```{r}
# import data
data = read_csv("./data/dataset.final.csv") %>% 
  select(-X1, -county_name, -state_abbr)
```

### Table 1: Descriptive Statistics for Continuous Variables (At the County Level)

```{r}
# For continuous variables

table_1 = data %>% 
  select(-community_health_center_ind, -hpsa_ind) %>% 
  skimr::skim_to_wide() %>% 
  as.data.frame() %>%
  select(2, 3, 5, 6:7, 9:11) %>% 
  rename(
    "NAs" = missing,
    "N" = n,
    "Mean" = mean,
    "Std. Dev." = sd,
    "1st Quartile" = p25,
    "Median" = p50,
    "3rd Quartile" = p75
  ) %>% 
  mutate(
    variable = str_replace(variable, "population_density", "Population Density"),
    variable = str_replace(variable, "population_size", "Population Size"),
    variable = str_replace(variable, "age_19_64", "Percent with Age 19-64"),
    variable = str_replace(variable, "age_19_under", "Percent with Age <19"),
    variable = str_replace(variable, "age_65_84", "Percent with Age 65-84"),
    variable = str_replace(variable, "age_85_and_over", "Percent with Age 85+"),
    variable = str_replace(variable, "ale", "Average Life Expectancy"),
    variable = str_replace(variable, "asian", "Percent Asian"),
    variable = str_replace(variable, "black", "Percent Black"),
    variable = str_replace(variable, "diabetes", "Percent with Diabetes"),
    variable = str_replace(variable, "disabled_medicare", "Percent Medicare Based on Disability"),
    variable = str_replace(variable, "elderly_medicare", "Percent Medicare Based on Elderly Status"),
    variable = str_replace(variable, "few_fruit_veg", "Percent with Few Fruits or Vegetables"),
    variable = str_replace(variable, "health_status", "Percent with Health Status"),
    variable = str_replace(variable, "high_blood_pres", "Percent with High Blood Pressure"),
    variable = str_replace(variable, "hispanic", "Percent Hispanic"),
    variable = str_replace(variable, "major_depression ", "Percent with Major Depression"),
    variable = str_replace(variable, "native_american", "Percent Native American"),
    variable = str_replace(variable, "no_hs_diploma", "Percent with No High School Diploma"),
    variable = str_replace(variable, "obesity", "Percent Obese"),
    variable = str_replace(variable, "poverty", "Percent Poverty"),
    variable = str_replace(variable, "prim_care_phys_rate", "Physician Rate"),
    variable = str_replace(variable, "recent_drug_use", "Percent with Recent Drug Use"),
    variable = str_replace(variable, "smoker", "Percent Smoking"),
    variable = str_replace(variable, "unemployed", "Percent Unemployed"),
    variable = str_replace(variable, "unhealthy_days", "Number of Monthly Unhealthy Day"),
    variable = str_replace(variable, "uninsured", "Percent Uninsured"),
    variable = str_replace(variable, "white", "Percent White")
    ) %>% 
  rename("Variable (Per County)" = variable) %>% 
  knitr::kable()
```

|Variable (Per County)                    |NAs  |Mean     |Std. Dev. |1st Quartile |Median |3rd Quartile |
|:----------------------------------------|:----|:--------|:---------|:------------|:------|:------------|
|Average Life Expectancy                  |0    |76.32    |2         |75           |76.5   |77.7         |
|Population Density                       |1    |250.07   |1703.27   |17           |44     |109.75       |
|Population Size                          |0    |94427.06 |306520.4  |11220        |25270  |64111        |
|Percent with Age <19                     |0    |24.81    |3.28      |22.7         |24.6   |26.4         |
|Percent with Age 19-64                   |0    |60.28    |3.35      |58.3         |60.3   |62.3         |
|Percent with Age 65-84                   |0    |12.79    |3.33      |10.7         |12.5   |14.7         |
|Percent with Age 85+                     |0    |2.12     |0.95      |1.5          |1.9    |2.6          |
|Percent White                            |0    |87.04    |16.14     |82.8         |94.1   |97.6         |
|Percent Black                            |0    |8.99     |14.55     |0.5          |2.1    |10.3         |
|Percent Native American                  |0    |1.95     |7.62      |0.2          |0.4    |0.9          |
|Percent Asian                            |0    |1.12     |2.76      |0.3          |0.5    |1            |
|Percent Hispanic                         |0    |7.02     |12.47     |1.1          |2.3    |6.3          |
|Death Rate                               |3    |905.64   |131.21    |814.25       |898.6  |989.8        |
|Percent with Health Status               |662  |17.32    |6.09      |12.9         |16.4   |20.9         |
|Number of Monthly Unhealthy Days         |0    |0.019    |0.36      |0.021        |0.025  |0.03         |
|Percent not Exercising                   |933  |26.51    |6.7       |21.9         |26     |30.8         |
|Percent with Few Fruits or Vegetables    |1235 |78.92    |5.16      |75.5         |79     |82.4         |
|Percent Obese                            |915  |24.15    |4.9       |21.1         |24.3   |27.2         |
|Percent with High Blood Pressure         |1617 |26.48    |5.44      |22.8         |26.2   |29.9         |
|Percent Smoking                          |872  |23.11    |5.73      |19.4         |23     |26.7         |
|Percent with Diabetes                    |420  |7.81     |2.76      |5.9          |7.5    |9.45         |
|Physician Rate                           |0    |57.6     |44.78     |30.55        |50.6   |74.7         |
|Dentist Rate                             |1    |32.19    |21.5      |18.7         |30     |43.3         |
|Percent Poverty                          |1    |13.35    |4.88      |9.8          |12.6   |16.2         |
|Percent Unemployed                       |543  |6.11     |1.34      |5.2          |6      |6.8          |
|Percent Uninsured                        |0    |14.0     |3.60      |11.1         |13.2   |17.0         |
|Percent Medicare Based on Disability     |0    |2.50     |0.79      |1.70         |2.30   |3.10         |
|Percent Medicare Based on Elderly Status |0    |9.80     |0.8       |11.0         |14.2   |17.1         |
|Percent with Major Depression            |0    |6.15     |0.66      |5.60         |5.90   |6.50         |
|Percent with No High School Diploma      |0    |15.0     |5.81      |11.3         |14.0   |19.2         |
|Percent with Recent Drug Use             |0    |5.21     |1.12      |4.55         |5.12   |5.73         |
|Percent Severely Disabled                |0    |2.56     |0.62      |2.03         |2.70   |3.46         |

</br>
</br>

```{r}
set.seed(3)
trRows <- createDataPartition(data$ale,
                              p = .75,
                              list = F)

data[2,34] = NA
x <- model.matrix(ale~., data)[,-1]
x = x[match(rownames(data), rownames(x)),]

n = dim(data)[2]
for (i in 2 : n){
  x[,names(data)[i]] = data[[i]]
}
rownames(x) = rownames(data)

y <- data$ale


#Train data
x1<-as.matrix(x)[trRows,]
y1<-data$ale[trRows]

#Test data
x2<-as.matrix(x)[-trRows,]
y2<-data$ale[-trRows]
```

```{r}
## `caret`

ctrl <- trainControl(method = "cv")
```

```{r}
### Least Square
set.seed(3)

ls.fit <- train(x1, y1,
                method = "lm",
                preProcess = c("center", "scale", "knnImpute"),
                trControl = ctrl)

# test MSE
pred_ls <- predict(ls.fit, newdata = x2)
ls_test = mean((pred_ls - y2)^2)
```

```{r}
### Ridge

set.seed(3)

ridge.fit <- train(x1, y1,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(-5, 2, length=200))),
                   preProcess = c( "center", "scale", "knnImpute"),
                   trControl = ctrl)

ridge_tuning <- plot(ridge.fit, xTrans = function(x1) log(x1)) 

# ridge.fit$bestTune

coef_ridge = coef(ridge.fit$finalModel,ridge.fit$bestTune$lambda)
# head(coef_ridge)

# test MSE
pred_rg = predict(ridge.fit, newdata = x2)
ridge_test = mean((pred_rg - y2)^2)
```


```{r}
### Lasso

set.seed(3)
lasso.fit <- train(x1, y1,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-4, -1, length=200))),
                   preProcess = c("center", "scale", "knnImpute"),
                   trControl = ctrl)

lasso_tuning <- plot(lasso.fit, xTrans = function(x1) log(x1))

trans = preProcess(x1, method = c("center", "scale", "knnImpute"))

coef_lasso = 
  predict(lasso.fit$finalModel, newx =  predict(trans,x2), 
                        s = lasso.fit$bestTune$lambda, type="coefficients")
# nrow(summary(coef_lasso))
# There are 15 non-zero coefficient estimates

# test MSE
pred_lasso <- predict(lasso.fit, newdata = x2)
lasso_test = mean((pred_lasso - y2)^2)
```


```{r}
# Obtain the number of non-zero coefficient estimates
coef_lasso = coef_lasso %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  rename('coefficient' = '1') 

non_zero_coef = coef_lasso %>% 
  filter(coefficient != 0)

# non_zero_coef %>% nrow() # 15 non-zero coefficient estimates

# non_zero_coef %>% knitr::kable() 
```


```{r PCR}
# PCR Model
set.seed(3)

pcr.fit <- train(x1, y1,
                 method = "pcr",
                 tuneLength = 33,
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)


pcr_tuning <- ggplot(pcr.fit, highlight = TRUE)


# test MSE
pred_pcr = predict(pcr.fit, newdata = x2)
pcr_test = mean((pred_pcr - y2)^2)
```


```{r pls}
# PLS
set.seed(3)

pls.fit <- train(x1, y1,
                 method = "pls",
                 tuneLength = 33,
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)


# summary(pls.fit)

pls_tuning <- ggplot(pls.fit, highlight = TRUE)+ theme_bw()

# test MSE
pred_pls = predict(pls.fit, newdata = x2)
pls_test = mean((pred_pls - y2)^2)
```


```{r gam}
### GAM

set.seed(3)

gam.fit <- train(x, y,
                 method = "gam",
                 tuneLength = data.frame(method = "GCV.Cp", select = c("TRUE", "FALSE")),
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)
# summary(gam.fit)
# gam.fit$bestTune
# plot(gam.fit$finalModel, pages = 4)

# test MSE
pred_gam = predict(gam.fit, newdata = x2)
gam_test = mean((pred_gam - y2)^2)
```


```{r}
### MARS

mars_grid <- expand.grid(degree = 1:2, 
                         nprune = 3:15)

set.seed(3)

mars.fit <- train(x1, y1,
                 method = "earth",
                 tuneGrid = mars_grid,
                 preProcess = c("center", "scale", "knnImpute"),
                 trControl = ctrl)

mars_tuning <- ggplot(mars.fit)
# summary(mars.fit)
# mars.fit$bestTune

# coef(mars.fit$finalModel)

# test MSE
pred_mars = predict(mars.fit, newdata = x2)
# test MSE
mars_test = mean((pred_mars - y2)^2)
```

```{r}
### KNN
set.seed(3)

# To find the optimal tuning parameter: 

# First I tried `tuneGrid = data.frame(k = seq(1,300,by = 5))`, the optimal k is 21. The cross-validation RMSE keep rising as k increases from 21 to 300 by adding increment number 5 for each time.

# Then I tried `tuneGrid = data.frame(k = seq(1,50,by = 1))`, generate the sequence from 1 to 40 by adding increment number 1 for each time, the optimal k is 8
knn.fit <- train(x1, y1,
                 method = "knn",
                 tuneGrid = data.frame(k = seq(1,40,by = 1)),
                 preProcess = c( "center", "scale", "knnImpute"),
                 trControl = ctrl)

# knn.fit$bestTune

knn_tuning <- ggplot(knn.fit)

# test MSE
pred.knn = predict(knn.fit, newdata = x2)
knn_test = mean((pred.knn - y2)^2)
```

```{r}
## Regression Tree
set.seed(3)
# tune over cp, method = "rpart"
rpart.fit <- train(x1, y1, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-9,-5, length = 20))), # (-6,-4), (-9,-5)
                   preProcess = c( "center", "scale", "knnImpute"),
                   trControl = ctrl)

rpart_tuning <- ggplot(rpart.fit, highlight = TRUE)
# rpart.plot(rpart.fit$finalModel)
# rpart.fit$bestTune

# test MSE
predy2.rt <- predict(rpart.fit, newdata =x2)
rpart_test = mean((predy2.rt - y2)^2) # 0.764
```

```{r}
## Bagging
bag.grid <- expand.grid(mtry = 33,
                       splitrule = "variance",
                       min.node.size = 1:15)
set.seed(3)
bag.fit <- train(x1, y1, 
                method = "ranger",
                tuneGrid = bag.grid,
                preProcess = c( "center", "scale", "knnImpute"),
                trControl = ctrl)

bagging_tuning <- ggplot(bag.fit, highlight = TRUE)

# bag.fit$bestTune

# test MSE
predy2.bag <- predict(bag.fit, newdata =x2)
bagging_test = mean((predy2.bag - y2)^2) # 0.605
```


```{r}
## Random Forest
rf.grid <- expand.grid(mtry = 1:33,
                       splitrule = "variance",
                       min.node.size = 1:15)
set.seed(3)
rf.fit <- train(x1, y1, 
                method = "ranger",
                tuneGrid = rf.grid,
                preProcess = c( "center", "scale", "knnImpute"), 
                importance = "impurity",
                trControl = ctrl)

rf_tuning <- ggplot(rf.fit, highlight = TRUE)

# rf.fit$bestTune

# barplot(sort(ranger::importance(rf.fit$finalModel), decreasing = FALSE),
#         las = 2, horiz = TRUE, cex.names = 0.7,
#         col = colorRampPalette(colors = c("darkred","white","darkblue"))(33))


# test MSE
pred_rf <- predict(rf.fit, newdata = x2)
rf_test = mean((pred_rf - y2)^2) # 0.595
```


```{r}
### Boosting

### 1st attempt ###
# gbm.grid <- expand.grid(
#   n.trees = c(2000,2500,3000,3500,4000,4500,5000),
#   interaction.depth = 2:10,
#   shrinkage = c(0.001,0.003,0.005), 
#   n.minobsinnode = 1)

# gbm.fit$bestTune
#    n.trees interaction.depth shrinkage n.minobsinnode
#      4500                10     0.003              1

gbm.grid4 <- expand.grid(  
  n.trees = seq(4300, 5000, by = 100), # number of trees
  interaction.depth = 8:16, # number of splits d in each tree
  shrinkage = c(0.003, 0.004, 0.005), # learnig rate              
  n.minobsinnode = 1) # the minimum number of obs in your node

set.seed(3)

gbm.fit4 <- train(x1, y1,
                 method = "gbm",
                 tuneGrid = gbm.grid4, 
                 trControl = ctrl, 
                 preProcess = c("center", "scale", "knnImpute"),  
                 verbose = FALSE)

# gbm.fit4$bestTune

gbm_tuning <- ggplot(gbm.fit4, highlight = T)

# summary(gbm.fit4$finalModel, las = 2, cBars = 33, cex.names = 0.6)

# test MSE
pred.gbm4 = predict(gbm.fit4, newdata = x2)

gbm_test4 = mean((pred.gbm4 - y2)^2) 

```


```{r}
# Model comparision by cross-validation RMSE
set.seed(3)

resamp <- resamples(list(ls = ls.fit,
                         ridge = ridge.fit, 
                         lasso = lasso.fit, 
                         pcr = pcr.fit,
                         pls = pls.fit,   
                         gam = gam.fit,
                         mars = mars.fit,
                         knn = knn.fit,
                         regression_tree = rpart.fit,
                         bagging = bag.fit,
                         random_forest = rf.fit,
                         boosting = gbm.fit4))


rmse.df <- resamp$values %>% 
  select(ends_with("~RMSE")) 


table.mse <- rmse.df %>% 
  gather(key = model, value = mse) %>% 
  mutate(model = str_replace(model, "~RMSE", ""),
         model = fct_reorder(model, mse, .desc = FALSE, .fun = mean)) %>%
  group_by(model) %>%
  summarize(
    mean_rmse = mean(mse),
    variance_rmse = sd(mse)^2,
    Q1 = quantile(mse, .25),
    median_rmse = median(mse),
    Q3 = quantile(mse, .75)
  ) 

```

### Table 2: Model Comparison by Cross-validation RMSE

```{r}
table.mse %>% 
  arrange(desc(mean_rmse)) %>%
  knitr::kable(digits = 4)
```


```{r}
# Compare test MSE
test_mse <- tibble(ls = ls_test,
       ridge = ridge_test,
       lasso = lasso_test,
       pcr = pcr_test,
       pls = pls_test,
       gam = gam_test,
       mars = mars_test, 
       knn = knn_test,
       regression_test = rpart_test,
       bagging = bagging_test,
       random_forest = rf_test,
       boosting = gbm_test4
) %>% 
  gather(key = "model", value = "test_MSE", ls:boosting)
```

</br>
</br>

### Figure 1: Correlation Plot

```{r fig.width = 15, fig.height = 15}
data_1 = data

x = model.matrix(ale ~., data_1)[,-1]

corrplot(cor(x), method = "number", number.cex = 0.5, tl.cex = 0.8)
```

```{r}
# unsupervised learning

data = read_csv("./data/dataset.final.csv") %>% 
  select(-X1, -county_name) %>% 
  select(state_abbr, everything()) %>% 
  na.omit() %>% 
  group_by(state_abbr) %>% 
  summarise_all(funs(mean))

state <- data.frame(state_abbr = state.abb,
                    state_name = state.name)

data = left_join(data, state, by = "state_abbr") %>% 
  select(state_abbr, state_name, ale, everything()) %>% 
  # state_name is factor
  mutate(state_name = as.character(state_name)) 

data$state_name[is.na(data$state_name)] <- "District of Columbia"


dat1 <- data[,3:36]
dat1 <- scale(dat1) # scaled
rownames(dat1) <- data$state_name
```

```{r}
# clustering
hc.complete <- hclust(dist(dat1), method = "complete")

clustering <- fviz_dend(hc.complete, k = 5,        
          cex = 0.5, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco")

# ind.complete <- cutree(hc.complete, 5)
```

```{r}
# heatmap

col1 <- colorRampPalette(brewer.pal(9, "GnBu"))(100)
col2 <- colorRampPalette(brewer.pal(3, "Spectral"))(2)

heatmap <- heatmap.2(t(dat1), 
          col = col1, keysize=.8, key.par = list(cex=.5),
          trace = "none", key = TRUE, cexCol = 0.75, 
          labCol = as.character(row.names(dat1)),
          margins = c(10, 10))
```

### Figure 2: Hierarchical Clustering at the State Level

```{r}
clustering
```

### Figure 3: Heatmap at the State Level

```{r fig.width = 15, fig.height = 15}
heatmap
```

### Figure 4: Violin Box Plot for Model Comparison by Cross-validation RMSE

```{r}
### Plots for RMSE across models
# Density Plot for RMSE
density.mse <- rmse.df %>% 
  gather(key = model, value = mse) %>% 
  mutate(model = str_replace(model, "~RMSE", ""),
         model = fct_reorder(model, mse, .desc = FALSE, .fun = mean)) %>% 
  ggplot(aes(x = mse, colour = model, fill = model)) + 
  geom_density(position = "stack", alpha = 0.3) + 
  labs(
    y = "Density",
    x = "Root Mean Square Error",
    title = sprintf("Model RMSE Comparison")
  ) +
  viridis::scale_fill_viridis(
    option = "magma",
    name = "MSE",
    begin = 1,
    end = 0,
    discrete = TRUE) +
    viridis::scale_colour_viridis(
    option = "magma",
    name = "MSE",
    begin = 1,
    end = 0,
    discrete = TRUE)

# Violin + Box Plot for RMSE
violin.mse <- rmse.df %>% 
  gather(key = model, value = mse) %>% 
  mutate(model = str_replace(model, "~RMSE", ""),
         model = fct_reorder(model, mse, .desc = FALSE, .fun = mean)) %>% 
  ggplot(aes(x = model, y = mse)) + 
  geom_violin(aes(fill = model), trim = FALSE, alpha = 0.3) + 
  geom_boxplot(width = 0.25) +
  labs(
    y = "Cross-validation RMSE",
    x = "Model"
  ) +
  viridis::scale_fill_viridis(
    option = "magma",
    name = "Model",
    begin = 1,
    end = 0,
    discrete = TRUE) +
  theme(
    legend.position = "none",
    axis.title.y = element_text(size = 20),
    axis.title.x = element_text(size = 20),
    axis.text.x = element_text(size = 20),
    axis.text.y = element_text(size = 20)
    ) +
  coord_flip()

```

```{r fig.width = 15, fig.height = 15}
violin.mse
```

### Figure 5: Partial Dependence Plot (PDP) Based on Random Forest

```{r}
# Partial Dependence Plot (PDP) for Random Forest

pdp_df <- data.frame(x1, ale = y1)

pdp_rf_1 <- rf.fit %>% 
  partial(pred.var = "all_death", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Death Rate",
       y = "Partial Dependence Function")

pdp_rf_2 <- rf.fit %>%          # selected
  partial(pred.var = "poverty", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Percent Poverty",
       y = "Partial Dependence Function") 

pdp_rf_3 <- rf.fit %>%     # selected
  partial(pred.var = "no_hs_diploma", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Percent with No High School Diploma",
       y = "Partial Dependence Function")

pdp_rf_5 <- rf.fit %>% 
  partial(pred.var = "black", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Percent Black",
       y = "Partial Dependence Function")

pdp_rf_6 <- rf.fit %>% 
  partial(pred.var = "health_status", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = x1) +
  labs(x = "Health Status",
       y = "Partial Dependence Function")

grid.arrange(pdp_rf_1, pdp_rf_2, pdp_rf_3, pdp_rf_5, pdp_rf_6, nrow = 2)
```

### Figure 6: Feature Effects in Two Different Counties Based on Random Forest

```{r}
# Plot the features in an explanation: plot_features()

# remain the county_name for data
data_county = read_csv("./data/dataset.final.csv") %>% 
  select(-X1) 

data_county_test <- data_county[-trRows, ]

# Check: if there are duplicate county
data_county_test %>% 
  group_by(county_name) %>% 
  count() %>% 
  arrange(desc(n)) # some state have counties with the same name

data_county_test %>% 
  group_by(county_name, state_abbr) %>% 
  count() %>% 
  arrange(desc(n)) 

data_county_test$county <- paste(data_county_test$county_name, data_county_test$state_abbr)

data_county_test = data_county_test %>% 
  select(-county_name, -state_abbr, -ale)

# Plot
library(lime)

# select the 200th county: Guthrie in IA
new_obs200 <- data_county_test[200, ] %>% 
  column_to_rownames("county") # row.names(new_obs) <- new_obs$county
# random forest
explainer.rf200 <- lime(data.frame(x1), rf.fit) # lime(predictors, model.fit)
explanation.rf200 <- explain(new_obs200, explainer.rf200, n_features = 8) # n_features = 8 or 5
rf_200 = plot_features(explanation.rf200) +
  ggtitle("Random Forest")

# select the 10th county: Lamar
new_obs10 <- data_county_test[10, ] %>% 
  column_to_rownames("county") # row.names(new_obs) <- new_obs$county
# random forest
explainer.rf10 <- lime(data.frame(x1), rf.fit) # lime(predictors, model.fit)
explanation.rf10 <- explain(new_obs10, explainer.rf10, n_features = 8) # n_features = 8 or 5
rf_10 = plot_features(explanation.rf10) +
  ggtitle("Random Forest")


# grid.arrange
grid.arrange(rf_200, rf_10, nrow = 1)

# select the 10th county: Lamar
# 10: Lamar in AL
# 15: Montgomery in AL
# 200: Guthrie in IA
# 300: Clare in MI
# 700: Fairfax in VA
```